{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nudging tendencies transects\n",
    "\n",
    "Make transects of nudging and physics tendencies from a coarse nudged run and compare to the fine-res apparent sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run predictive_mapper.py\n",
    "import os\n",
    "import intake\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import fsspec\n",
    "import yaml\n",
    "from datetime import timedelta\n",
    "from dask.diagnostics import ProgressBar\n",
    "from matplotlib import rc, pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "rc('animation', html='html5')\n",
    "import matplotlib as mpl\n",
    "from IPython.display import HTML\n",
    "import fv3fit\n",
    "from fv3fit._shared import EnsembleModel\n",
    "import loaders\n",
    "from vcm.catalog import catalog as CATALOG\n",
    "from vcm.safe import get_variables\n",
    "from vcm.calc import thermo\n",
    "from vcm.fv3.metadata import standardize_fv3_diagnostics\n",
    "from vcm import encode_time, interpolate_unstructured, convert_timestamps\n",
    "from loaders.mappers import GeoMapper\n",
    "from typing import Mapping, Sequence, Union, Tuple\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams.update(**{\n",
    "    'font.size': 8,\n",
    "    'xtick.labelsize': 'small',\n",
    "    'ytick.labelsize': 'small',\n",
    "    'axes.labelsize': 'small',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N2F_TRAIN_TEST_DATA_URL = 'gs://vcm-ml-experiments/2021-04-13-n2f-c3072/3-hrly-ave-rad-precip-setting-30-min-rad-timestep-shifted-start-tke-edmf-3-hrly-ave-physics-tendencies'\n",
    "N2F_MODEL_URL = {\n",
    "    '$TqR$-RF':'gs://vcm-ml-experiments/2021-06-21-nudge-to-c3072-dq1-dq2-only/rf/trained_models/postphysics_ML_tendencies',\n",
    "    '$TqR$-NN':'gs://vcm-ml-experiments/2021-05-11-nudge-to-c3072-corrected-winds/nn-ensemble-model/trained_models/dq1-dq2',\n",
    "} \n",
    "PLOT_VARS=[\n",
    "    'pressure',\n",
    "    '{var}_tendency_due_to_nudging',\n",
    "    '{var}',\n",
    "    '{var}_tendency_due_to_ML'\n",
    "]\n",
    "STATE_VARS = [\n",
    "    'pressure_thickness_of_atmospheric_layer',\n",
    "    'vertical_thickness_of_atmospheric_layer',\n",
    "    'surface_geopotential',\n",
    "    'specific_humidity',\n",
    "    'air_temperature',\n",
    "]\n",
    "Q1_SCALE = dict(vmin=-5, vmax=5)\n",
    "TIMESTEP_SECONDS = 10800\n",
    "SECONDS_PER_DAY = 86400\n",
    "G_PER_KG = 1000\n",
    "INSTANTANEOUS_TIMESTAMP = '20160904.143000'\n",
    "OUTDIR = 'figures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tendency_units(ds):\n",
    "    for var in ds.data_vars:\n",
    "        ds[var] = ds[var] * TIMESTEP_SECONDS\n",
    "        if \"specific_humidity\" in var:\n",
    "            ds[var] = ds[var] * G_PER_KG\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(ds, freq='3H'):\n",
    "    ds = ds.resample({'time': freq}, base=1, loffset=timedelta(minutes=90)).mean()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nudge-to-fine run dataset\n",
    "nudging_tendencies = tendency_units(standardize_fv3_diagnostics(\n",
    "    intake.open_zarr(os.path.join(N2F_TRAIN_TEST_DATA_URL, 'nudging_tendencies.zarr'), consolidated=True).to_dask()\n",
    "))\n",
    "physics_tendencies = tendency_units(standardize_fv3_diagnostics(\n",
    "    intake.open_zarr(os.path.join(N2F_TRAIN_TEST_DATA_URL, 'physics_tendencies.zarr'), consolidated=True).to_dask()\n",
    "))\n",
    "states = time_average(standardize_fv3_diagnostics(\n",
    "    get_variables(\n",
    "        intake.open_zarr(os.path.join(N2F_TRAIN_TEST_DATA_URL, 'state_after_timestep.zarr'), consolidated=True).to_dask(),\n",
    "        STATE_VARS\n",
    "    )\n",
    "))   \n",
    "run_dataset = xr.merge([nudging_tendencies, physics_tendencies, states], join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heights_and_pressures(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \n",
    "    ds = ds.assign({\n",
    "        'pressure': 0.01*thermo.pressure_at_midpoint_log(\n",
    "            ds['pressure_thickness_of_atmospheric_layer'],\n",
    "            dim='z'\n",
    "        ).assign_attrs({\n",
    "            'long_name': \"pressure at layer center\",\n",
    "            'units': 'hPa'\n",
    "        })\n",
    "    })\n",
    "    \n",
    "    ds = ds.assign({\n",
    "        'pressure_interface': 0.01*thermo.pressure_at_interface(\n",
    "            ds['pressure_thickness_of_atmospheric_layer'],\n",
    "            dim_center='z',\n",
    "            dim_outer='zb'\n",
    "        ).assign_attrs({\n",
    "            'long_name': \"pressure at layer interface\",\n",
    "            'units': 'hPa'\n",
    "        })\n",
    "    })\n",
    "    \n",
    "    ds = ds.assign({\n",
    "        'height': thermo.height_at_midpoint(\n",
    "            ds['vertical_thickness_of_atmospheric_layer'],\n",
    "            ds['surface_geopotential'],\n",
    "            dim='z'\n",
    "        ).assign_attrs({\n",
    "            'long_name': \"height at layer center\",\n",
    "            'units': 'm'\n",
    "        })\n",
    "    })\n",
    "    \n",
    "    ds = ds.assign({\n",
    "        'height_interface': thermo.height_at_interface(\n",
    "            ds['vertical_thickness_of_atmospheric_layer'],\n",
    "            ds['surface_geopotential'],\n",
    "            dim_center='z',\n",
    "            dim_outer='zb'\n",
    "        ).assign_attrs({\n",
    "            'long_name': \"height at layer interface\",\n",
    "            'units': 'm'\n",
    "        })\n",
    "    })\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dataset = add_heights_and_pressures(run_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open grid and merge, overriding grid vars in fortran diagnostics because repeated values in time mess with the kdtree\n",
    "grid_c48 = standardize_fv3_diagnostics(CATALOG[\"grid/c48\"].to_dask())\n",
    "run_dataset = xr.merge([grid_c48, run_dataset], compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models for making ML predictions\n",
    "\n",
    "models = {}\n",
    "for ml_type, url in N2F_MODEL_URL.items():\n",
    "    if 'RF' in ml_type:\n",
    "        models[ml_type] = fv3fit.load(url)\n",
    "    else:\n",
    "        # have to go around fv3fit loading the ensemble model because of bug\n",
    "        with fsspec.open(os.path.join(url, 'ensemble_model.yaml'), \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        ensemble_members = [fv3fit.load(path) for path in config[\"models\"]]\n",
    "        reduction = config[\"reduction\"]\n",
    "        models[ml_type] = EnsembleModel(ensemble_members, reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nudging_variables = [\n",
    "    \"air_temperature\",\n",
    "    \"specific_humidity\",\n",
    "    \"x_wind\",\n",
    "    \"y_wind\",\n",
    "    \"pressure_thickness_of_atmospheric_layer\"\n",
    "]\n",
    "\n",
    "predictive_mapper = PredictiveMapper(\n",
    "    models,\n",
    "    N2F_TRAIN_TEST_DATA_URL,\n",
    "    loaders.mappers.open_nudge_to_fine,\n",
    "    {'nudging_variables': nudging_variables},\n",
    "    grid_c48\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TendencyCrossSectionMapper(GeoMapper):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: xr.Dataset,\n",
    "        preditive_mapping: Mapping[str, xr.Dataset],\n",
    "        transect: Mapping[str, xr.Dataset],\n",
    "        primary_var: str,\n",
    "        other_vars: Sequence[str],\n",
    "        title: str,\n",
    "        units: str='kg/kg/s',\n",
    "        scale: float=25\n",
    "    ):\n",
    "        self._dataset = dataset\n",
    "        self._predictive = preditive_mapping\n",
    "        self.primary_var = primary_var\n",
    "        self._other_vars = other_vars\n",
    "        self._transect = transect\n",
    "        self.title = title\n",
    "        self.units = units\n",
    "        self.scale = scale\n",
    "        \n",
    "        times = self._dataset.time.values.tolist()\n",
    "        time_strings = [encode_time(time) for time in times]\n",
    "        self._time_lookup = dict(zip(time_strings, times))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, key: Union[str, slice]) -> xr.Dataset:\n",
    "        \n",
    "        if isinstance(key, str):\n",
    "            ds = self._get_timestep(key)\n",
    "        elif isinstance(key, slice):\n",
    "            ds = self._get_timeslice(key)\n",
    "        ds = get_variables(ds, [self.primary_var] + self._other_vars)\n",
    "        with ProgressBar():\n",
    "            print(f'Loading data for {key}')\n",
    "            return ds.load()\n",
    "        \n",
    "    def _get_timestep(self, key: str) -> Tuple[xr.Dataset, xr.Dataset]:\n",
    "        predicted = tendency_units(self._predictive[key])\n",
    "        nudged = (\n",
    "            self._dataset\n",
    "            .sel(time=self._time_lookup[key])\n",
    "            .drop_vars('time').expand_dims({'time': [key]})\n",
    "        )\n",
    "        ds = xr.merge([nudged, predicted], compat='override')\n",
    "        return interpolate_unstructured(ds, self._transect).expand_dims({'time': ds.time})\n",
    "    \n",
    "    def _get_timeslice(self, key_slice: slice) -> Tuple[xr.Dataset, xr.Dataset]:\n",
    "        timesteps = sorted(self._time_lookup.keys())\n",
    "        start = timesteps.index(key_slice.start) if key_slice.start is not None else None\n",
    "        end = timesteps.index(key_slice.stop) if key_slice.stop is not None else None\n",
    "        step = key_slice.step if key_slice.step is not None else None\n",
    "        timestep_subset = timesteps[start:end:step]\n",
    "        predicted = tendency_units(xr.concat(\n",
    "            [self._predictive[timestep] for timestep in timestep_subset],\n",
    "            dim='time'\n",
    "        ))\n",
    "        nudged = (\n",
    "            self._dataset\n",
    "            .isel(time=slice(start, end, step))\n",
    "            .assign_coords({'time': timestep_subset})\n",
    "        )\n",
    "        ds = xr.merge([nudged, predicted], compat='override')\n",
    "        return interpolate_unstructured(ds, self._transect)\n",
    "        \n",
    "    def keys(self):\n",
    "        return self._time_lookup.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transect_frame(time, ds, axes, var, pcolor_kwargs, title, yscale):\n",
    "\n",
    "    LEVELS, UNITS_CONV, NUM, LETTER = (\n",
    "        (range(0, 50, 5), G_PER_KG, '2', 'q')\n",
    "        if var == 'specific_humidity' else (range(200, 300, 20), 1, '1', 'T')\n",
    "    )\n",
    "    ds = ds.sel(time=time)\n",
    "    \n",
    "    ax0 = axes[0]\n",
    "    ax0.clear()\n",
    "    ax0.pcolormesh(\n",
    "        ds['transect'],\n",
    "        ds['pressure'],\n",
    "        ds[f'{var}_tendency_due_to_nudging'],\n",
    "        shading='nearest',\n",
    "        **pcolor_kwargs\n",
    "    )\n",
    "    ax0.set_xlabel('latitude')\n",
    "    ax0.set_ylim([1e3, 2e2])\n",
    "    ax0.set_ylabel('pressure [hPa]')\n",
    "    ax0.set_facecolor('olive')\n",
    "    ax0.set_title(f'a) nudging ($\\Delta Q_{{{LETTER}}})$')\n",
    "    derivations = [derivation for derivation in ds.derivation.values if derivation != 'target']\n",
    "    for i, derivation in enumerate(derivations):\n",
    "        axi = axes[1 + i]\n",
    "        axi.clear()\n",
    "        axi.pcolormesh(\n",
    "            ds['transect'],\n",
    "            ds['pressure'],\n",
    "            ds[f'{var}_tendency_due_to_ML'].sel(derivation=derivation),\n",
    "            shading='nearest',\n",
    "            **pcolor_kwargs\n",
    "        )\n",
    "        axi.invert_yaxis()\n",
    "        axi.set_ylim([1e3, 2e2])\n",
    "        axi.tick_params(labelleft=False)\n",
    "        axi.set_facecolor('olive')\n",
    "        axi.set_title(f'{string.ascii_lowercase[1 + i]}) {derivation} ($\\Delta Q_{{{LETTER}}}^{{ML}}$)')\n",
    "        axi.set_xlabel('latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_axes(pcolor_kwargs, primary_var, units, time):\n",
    "    fig = plt.figure()\n",
    "    axes = list(fig.subplots(1, 3, sharex=True).flatten())\n",
    "    norm = mpl.colors.Normalize(vmin=pcolor_kwargs['vmin'], vmax=pcolor_kwargs['vmax'])\n",
    "    sm = plt.cm.ScalarMappable(cmap=pcolor_kwargs['cmap'], norm=norm)\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(\n",
    "        sm,\n",
    "        ax=axes,\n",
    "        label=f\"{primary_var.replace('_', ' ')} tendency at {time} [{units}]\",\n",
    "        location='bottom',\n",
    "        aspect=50\n",
    "    )\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantaneous_transect(transect_mapper, time, fig_size=[10, 8], pcolor_kwargs=None):\n",
    "\n",
    "    pcolor_kwargs = pcolor_kwargs or {}\n",
    "    if 'vmin' not in pcolor_kwargs:\n",
    "        pcolor_kwargs['vmin'] = -2\n",
    "    if 'vmax' not in pcolor_kwargs:\n",
    "        pcolor_kwargs['vmax'] = 2\n",
    "    if 'cmap' not in pcolor_kwargs:\n",
    "        pcolor_kwargs['cmap'] = 'seismic'\n",
    "\n",
    "    fig, axes = get_frame_axes(pcolor_kwargs, transect_mapper.primary_var, transect_mapper.units, time)\n",
    "\n",
    "    ds = transect_mapper[time]\n",
    "    _transect_frame(time, ds, axes, transect_mapper.primary_var, pcolor_kwargs, transect_mapper.title, transect_mapper.scale)\n",
    "\n",
    "    fig.set_size_inches(fig_size)\n",
    "    fig.savefig(f\"{OUTDIR}/Figure_4_{transect_mapper.title.replace(' ', '_')}_{time}.eps\", bbox_inches='tight', facecolor='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meridional_transect(lon, lat_start, lat_stop, lat_res):\n",
    "    lat = np.arange(lat_start, lat_stop, lat_res)\n",
    "    lon = np.ones_like(lat) * lon\n",
    "    return {\n",
    "        'lat': xr.DataArray(lat, dims=['transect'], coords={'transect': lat}),\n",
    "        'lon': xr.DataArray(lon, dims=['transect'], coords={'transect': lat})\n",
    "    }\n",
    "\n",
    "def zonal_transect(lat, lon_start, lon_stop, lon_res):\n",
    "    lon = np.arange(lon_start, lon_stop, lon_res)\n",
    "    lat = np.ones_like(lon) * lat\n",
    "    return {\n",
    "        'lat': xr.DataArray(lat, dims=['transect'], coords={'transect': lon}),\n",
    "        'lon': xr.DataArray(lon, dims=['transect'], coords={'transect': lon})\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sahara_transect = meridional_transect(0, 0, 42, 1)\n",
    "var = 'specific_humidity'\n",
    "sahara_moisture_tendency_xs_mapper = TendencyCrossSectionMapper(\n",
    "    run_dataset,\n",
    "    predictive_mapper,\n",
    "    sahara_transect,\n",
    "    var,\n",
    "    [var_template.format(var=var, num=VAR_MAPPING[var]) for var_template in PLOT_VARS],\n",
    "    'Sahara moisture transect',\n",
    "    units='g/kg/3-hr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instantaneous_transect(sahara_moisture_tendency_xs_mapper, INSTANTANEOUS_TIMESTAMP, fig_size=[7.6, 3.74])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this converts matplotlib eps files to a more manageable size\n",
    "\n",
    "!epstopdf figures/Figure_4_Sahara_moisture_transect_20160904.143000.eps\n",
    "!pdftops -eps figures/Figure_4_Sahara_moisture_transect_20160904.143000.pdf\n",
    "!rm figures/Figure_4_Sahara_moisture_transect_20160904.143000.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fv3net",
   "language": "python",
   "name": "fv3net"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
